{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "477da81d-4467-4083-b1c2-049e60bcb77a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# INTRODUCTION TO LAKEFLOW DECLARATIVE PIPELINES (DATABRICKS)\n",
    "> Welcome to the world of simplified data engineering on Databricks! Declarative Pipelines, an integral part of Databricks Lakeflow (formerly known and powered by Delta Live Tables - DLT technology), change how we build data pipelines.\n",
    "\n",
    "Instead of writing complex code to manage orchestration, error handling, and infrastructure, you simply declare how data should flow and be transformed. Lakeflow handles the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d4b8763-699c-4bb2-9ffc-afb1a613f08f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# KEY CONCEPTS\n",
    "To understand declarative pipelines, you need to know a few basic concepts:\n",
    "\n",
    "- Pipeline: It's the complete graph of your data transformations. You configure the source, the notebooks (with the logic), and the destination.\n",
    "- Table: The basic unit of a pipeline. In DLT, there are two main types of tables (datasets):\n",
    "  - Live Table (Materialized Table): Similar to a \"Materialized View\". The transformation results are physically stored. It is updated incrementally.\n",
    "  - Live View (View): Similar to a traditional \"View\". The transformation is calculated \"on-the-fly\" whenever it's queried. It doesn't store data physically but is useful for breaking down complex logic.\n",
    "- Source: Where your raw data comes from. It can be cloud storage (S3, ADLS, GCS), Kafka, or other Delta tables.\n",
    "- Expectations: Data quality rules that you can apply directly to your tables. DLT collects metrics on how many records fail these rules and allows you to decide what to do with them (e.g., drop, keep, or stop the pipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e1cfddf3-fda2-4484-9589-fcc040f52b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# HOW DOES IT WORK?\n",
    "The creation process is quite straightforward:\n",
    "\n",
    "- Write the Code: You define your transformations in SQL or Python within one or more Databricks Notebooks.\n",
    "- Use Declarative Syntax: Instead of complex INSERT or MERGE commands, you use simple constructs.\n",
    "\n",
    "> In Python, you use decorators like @dlt.table and @dlt.view.\n",
    "\n",
    "> In SQL, you use syntax like CREATE OR REFRESH LIVE TABLE.\n",
    "\n",
    "- Configure the Pipeline: In the Databricks UI, you create a new Pipeline, point it to your notebooks, define the storage location, and configure the execution mode (continuous or scheduled).\n",
    "- Run: Lakeflow analyzes your code, builds the dependency graph (lineage), and manages all the processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef9215c-d67c-4a59-94a6-f0ad1be20a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ADVANTAGES OF USING DECLARATIVE PIPELINES\n",
    "\n",
    "- Simplicity: You focus on the transformation logic (SQL/Python), not on managing clusters, complex schedules, or dependencies.\n",
    "- Integrated Data Quality: \"Expectations\" allow you to clean and validate data directly in the table definition.\n",
    "- Incremental Processing: Lakeflow automatically manages incremental (streaming) processing, processing only new data, which is much more efficient.\n",
    "- Automatic Recovery: If an execution fails, the pipeline knows exactly where it left off (checkpoints) and resumes from that point.\n",
    "- Visibility (Lineage): The DLT interface shows you a visual graph of how your data flows between tables, making debugging and understanding easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d576634-6c85-46f0-89c1-b4c506f6df53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "GetStartedWithDatabricksEngineering_LakeFlowDeclarativePipelines_09_12",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}