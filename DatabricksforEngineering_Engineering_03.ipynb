{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e591d83",
   "metadata": {},
   "source": [
    "# USING DATABRICKS FOR DATA ENGINEERING\n",
    "## Module Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e6d483",
   "metadata": {},
   "source": [
    "# LEARNING OBJECTIVES\n",
    "- Describe the primary responsibilities and core skills of a data engineer \n",
    "- Explain the stages of a traditional data engineering architecture, from data ingestion to processing, storage, and consumption\n",
    "- Define the core components of the Databricks Lakeflow offering and describe how it benefits data engineering practitioners\n",
    "- Describe the benefits of Delta Lake and Delta tables in the Databricks Data Intelligence Platform\n",
    "- List the available data ingestion techniques in the Databricks Data Intelligence Platform \n",
    "- Explain the puporse of the Medallion Architecture for data transformation\n",
    "- Describe how Lakeflow Jobs and Lakeflow Declarative Pipelines work to create a unified orchestration offering within the plataform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3eba7",
   "metadata": {},
   "source": [
    "|USING DATABRICKS FOR DATA ENGINEERING (PART 1) |TIME |LECTURE |DEMO |LAB|\n",
    "|---|---|---|---|---|\n",
    "|Introduction to Data Engineering|6 mins| x | | |\n",
    "|Lakeflow Overview|2 mins| x | | |\n",
    "|Delta Lake Overview|7 mins| x | | |\n",
    "|Creating and Working with a Delta Table|22 mins| | x | |\n",
    "|Lakeflow Connect Ingestion Techniques Overview|3 mins| x | | |\n",
    "|Ingesting Data into Delta Lake|18 mins| | x | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6c0a7",
   "metadata": {},
   "source": [
    "|USING DATABRICKS FOR DATA ENGINEERING (PART 2) |TIME |LECTURE |DEMO |LAB|\n",
    "|---|---|---|---|---|\n",
    "|Data Transformation Overview|6 mins| x |  |  |\n",
    "|Transforming Data Using the Medallion Architecture|13 mins|  | x |  |\n",
    "|Performing ETL with Lakeflow Declarative Pipelines|2 mins| x |  |  |\n",
    "|Unified Orchestration Using Lakeflow Jobs|2 mins| x |  |  |\n",
    "|Creating a Simple Databricks Lakeflow Job|14 mins|  | x |  |\n",
    "|Ingest and Manipulate a Delta Table Lab|25 mins|  |  | x |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0755d",
   "metadata": {},
   "source": [
    "%md\n",
    "# INTRODUCTION TO DATA ENGINEERING\n",
    "\n",
    "## KEY RESPONSSABILITIES\n",
    "\n",
    "### TRANSFORMING RAW DATA TOE CLEAN, RELIABLE DATA\n",
    "- Data extraction from diverse sources\n",
    "- Cleasing to remove erros and inconsistencies\n",
    "- Data transformation to convert it to a structured and usable format for consumers\n",
    "\n",
    "## ENSURING THE QUALITY AND INTEGRITY OF DATA \n",
    "- Develop processes to monitor and maintain data accuracy, consistency, and reliability.\n",
    "- Keeps the data trustworthy and dependable.\n",
    "\n",
    "## DESIGN, BUILD, AND MAINTAIN DATA PIPELINES\n",
    "- Pathways through which data flows from various sources to our storage systems and analytical tools.\n",
    "- Create, optimize and automate these pipelines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa63a1f8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
