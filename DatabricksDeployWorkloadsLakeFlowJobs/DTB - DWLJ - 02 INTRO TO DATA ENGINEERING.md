2810250709
**Descrição:** Intro to Data Engineering with Databricks
**TAG:** #Databricks #COURSE #engineering #LAKEFLOW #JOBS 

# DTB - DWLJ - 02 INTRO TO DATA ENGINEERING
---


## DATA ENGINERRING WITH DATABRICKS

![[Pasted image 20251028071054.png]]

> It all begins with optimized storage using Delta Lake, Parquet, or Iceberg.

Built on top of this storage layer is unified governance with Unity Catalog. Unity Catalog is a centralized data catalog that provides access control, auditing, data lineage, quality monitoring, and data discovery across Databricks workspaces.

Databricks then offers Lakeflow, an end-to-end data engineering solution that empowers data engineers, software developers, SQL developers, analysts, and data scientists to deliver high-quality data for downstream analytics, Al, and operational applications. Lakeflow provides a unified platform for data ingestion, transformation, and orchestration, and includes the following components:

- **LAKEFLOW CONNECT:** A set of efficient ingestion connectors that simplify data ingestion from popular enterprise applications, databases, cloud storage, message buses, and local files.

- **LAKEFLOW DECLARATIVE PIPELINES:** A framework for building batch and streaming data pipelines using SQL and Python, designed to accelerate ETL development.

- **LAKEFLOW JOBS:** A workflow automation tool for Databricks that orchestrates data processing workloads. It enables coordination of multiple tasks within complex workflows, allowing for the scheduling, optimization, and management of repeatable processes.

![[Pasted image 20251028071659.png]]


```markdown
title: Focus
In this course, we're focusing specifically on Jobs - the orchestration component. Jobs allows you to orchestrate every type of workload within Databricks, including notebooks, SQL queries, dashboards, pipelines, and much more.
``` 

This unified approach is what makes Lakeflow Jobs so powerful.

# REFERÊNCIAS
---
[[DTB - DEPLOY WORKLOADS WITH LAKEFLOW JOBS]]