{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d9a9cde-b0e2-48d1-851e-740a56eb3f5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img\n",
    "    src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\"\n",
    "    alt=\"Databricks Learning\"\n",
    "  >\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ac2f615-a61b-45f8-81f7-d0a2db2a9233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Scaling Pandas with Spark\n",
    "\n",
    "\n",
    "<!-- ## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png)  -->\n",
    "\n",
    "In this lesson you:\n",
    "- Demonstrate the similarities of the pandas API on Spark API with the pandas API\n",
    "- Understand the differences in syntax for the same DataFrame operations in pandas API on Spark vs PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "184a76f9-0c25-4487-9f70-76a6e2f7d293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "Follow these steps to select the classic compute cluster:\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "    - In the drop-down, select **More**.\n",
    "    - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c56ce586-5c23-49e7-8b08-48e0b43b6267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Recall from the cloud computing lesson the open source technology [Apache Spark](https://spark.apache.org/), which is an open-source data processing engine that manages distributed processing of large data sets. \n",
    "\n",
    "In this course, we have been using Pandas, which is a great library for data analysis, but it only runs on one machine, it is not distributed. Luckily, we can use Spark just like traditional Pandas with the Pandas API on Spark. \n",
    "\n",
    "The pandas API on Spark project makes data scientists more productive when interacting with big data, by implementing the pandas DataFrame API on top of Apache Spark. By unifying the two ecosystems with a familiar API, pandas API on Spark offers a seamless transition between small and large data. \n",
    "\n",
    "See this <a href=\"https://databricks.com/blog/2021/10/04/pandas-api-on-upcoming-apache-spark-3-2.html\" target=\"_blank\">blog post</a> for more information on using Pandas syntax on Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b583160-68ef-4abf-b281-1e91a4cf5c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<!-- <div style=\"img align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://files.training.databricks.com/images/301/31gb.png\" width=\"900\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"img align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://files.training.databricks.com/images/301/95gb.png\" width=\"900\"/>\n",
    "</div> -->\n",
    "\n",
    "**Pandas** DataFrames are mutable, eagerly evaluated, and maintain row order. They are restricted to a single machine, and are very performant when the data sets are small.\n",
    "\n",
    "**Spark** DataFrames are distributed, lazily evaluated, immutable, and do not maintain row order. They are very performant when working at scale.\n",
    "\n",
    "**pandas API on Spark** provides the best of both worlds: pandas API with the performance benefits of Spark. However, it is not as fast as implementing your solution natively in Spark, and let's see why below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5757b9c2-3b52-411a-9c70-71da1216f987",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## InternalFrame\n",
    "\n",
    "The InternalFrame holds the current Spark DataFrame and internal immutable metadata.\n",
    "\n",
    "It manages mappings from pandas API on Spark column names to Spark column names, as well as from pandas API on Spark index names to Spark column names. \n",
    "\n",
    "If a user calls some API, the pandas API on Spark DataFrame updates the Spark DataFrame and metadata in InternalFrame. It creates or copies the current InternalFrame with the new states, and returns a new pandas API on Spark DataFrame.\n",
    "\n",
    "<!-- <div style=\"img align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://files.training.databricks.com/images/301/InternalFramePs.png\" width=\"900\"/>\n",
    "</div> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70a958d1-0a8f-4d06-8cda-7be426d2686e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read in the dataset\n",
    "\n",
    "* PySpark\n",
    "* pandas\n",
    "* pandas API on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4977184b-04b8-4b7e-a4aa-e94c9589ae31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"./Includes/Classroom-Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "220e6448-0e15-4756-b7be-4f4469532074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Just like how we can read in data from a CSV file in pandas, we can read in data from a file in spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffaf928a-bc0e-462f-82c9-c488469042d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv(f\"{DA.paths.datasets}/sf-airbnb/sf-airbnb.csv\", header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "display(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e8cf144-d0c2-4f71-b520-6821a7a0e8b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read in CSV with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a284800d-fa53-473a-80c5-3e625b13b6a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.read_csv(f\"{DA.paths.datasets}/sf-airbnb/sf-airbnb.csv\".replace(\"dbfs:\", \"/dbfs\"))\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "193a8cf9-68be-4a72-a75c-06e893af616c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read in CSV with pandas API on Spark. You'll notice pandas API on Spark generates an index column for you, like in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5affd35-9177-4a1d-aae0-1153f1acc92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "df = ps.read_csv(f\"{DA.paths.datasets}/sf-airbnb/sf-airbnb.csv\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7192c33a-207b-48e0-aed2-446b19ef0dce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### <a href=\"https://koalas.readthedocs.io/en/latest/user_guide/options.html#default-index-type\" target=\"_blank\">Index Types</a>\n",
    "\n",
    "![](https://files.training.databricks.com/images/301/koalas_index.png)\n",
    "\n",
    "Our recommended rule of thumb for choosing is as follows:\n",
    "* If your data is small, or if you're running workloads on a single-node cluster, then use **sequence**.\n",
    "* For larger datasets or multi-node cluster processing, try **distributed**.\n",
    "* If workloads are not working correctly or not delivering expected results, then use **distributed-sequence** for best compatibility while sacrificing some performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b0310a5-0197-4385-a162-b655314bf445",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ps.set_option(\"compute.default_index_type\", \"distributed-sequence\")\n",
    "df_dist_sequence = ps.read_csv(f\"{DA.paths.datasets}/sf-airbnb/sf-airbnb.csv\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n",
    "df_dist_sequence.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29244c1c-f274-499f-b529-66135869915e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Converting to pandas API on Spark DataFrame to/from Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36361eb4-f502-4dcc-b68c-ee47e239c72f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating a pandas API on Spark DataFrame from PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc959cbf-055a-41ec-9671-e1c549d44cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = ps.DataFrame(spark_df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41b1c929-24e2-48aa-a1d5-619112e7d93a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Alternative way of creating a pandas API on Spark DataFrame from PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "439fc5b9-b11e-4f72-94a9-744c99e845f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark_df.to_pandas_on_spark()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b81eeee-0581-4f24-bd03-5451d9edb476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Go from a pandas API on Spark DataFrame to a Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2762606-428d-4b20-86a2-1ac8d7bae950",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.to_spark())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "580517b7-70a8-49a8-9fa8-5b7a519993f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Value Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e2633f7-e880-4633-9e2b-43ffbfe24ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Get value counts of the different property types with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a18c52a-89bd-4eb9-8cca-2a614e96cf52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark_df.groupby(\"property_type\").count().orderBy(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7bc2789-d84f-40c4-aedf-6ec925a92a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Get value counts of the different property types with pandas API on Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fecfc72-3940-4426-bda1-83918f028895",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"property_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12c2aaaf-5e6b-4a0e-9759-730460ca05e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Visualizations\n",
    "\n",
    "Based on the type of visualization, the pandas API on Spark has optimized ways to execute the plotting.\n",
    "<!-- <br><br>\n",
    "\n",
    "![](https://files.training.databricks.com/images/301/ps_plotting.png) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52540f12-3339-4841-8a0f-a8bb4f37e1c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"bedrooms\"].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d1c317b-c6b8-4fae-9fb9-f731d233cec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SQL on pandas API on Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2bacea3-1b6e-4d42-a9a7-776afd9787f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ps.sql(\"SELECT distinct(property_type) FROM {df}\", df=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f43d26d0-ebf2-4bff-9dac-3c5a71050075",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Interesting Facts\n",
    "\n",
    "* With pandas API on Spark you can read from Delta Tables and read in a directory of files\n",
    "* If you use apply on a pandas API on Spark DF and that DF is <1000 (by default), pandas API on Spark will use pandas as a shortcut - this can be adjusted using **`compute.shortcut_limit`**\n",
    "* When you create bar plots, the top n rows are only used - this can be adjusted using **`plotting.max_rows`**\n",
    "* How to utilize **`.apply`** <a href=\"https://koalas.readthedocs.io/en/latest/reference/api/databricks.koalas.DataFrame.apply.html#databricks.koalas.DataFrame.apply\" target=\"_blank\">docs</a> with its use of return type hints similar to pandas UDFs\n",
    "* How to check the execution plan, as well as caching a pandas API on Spark DF (which aren't immediately intuitive)\n",
    "* Koalas are marsupials whose max speed is 30 kph (20 mph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40ebf041-2375-4f47-bcd1-1f11b7594f9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2025 Databricks, Inc. All rights reserved. Apache, Apache Spark, Spark, the Spark Logo, Apache Iceberg, Iceberg, and the Apache Iceberg logo are trademarks of the <a href=\"https://www.apache.org/\" target=\"_blank\">Apache Software Foundation</a>.<br/><br/><a href=\"https://databricks.com/privacy-policy\" target=\"_blank\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\" target=\"_blank\">Terms of Use</a> | <a href=\"https://help.databricks.com/\" target=\"_blank\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "ITP 13 - Scaling Pandas with Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}